# -*- coding: utf-8 -*-
"""Time Series Forecasting with LSTM: DOM Hourly Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sBt1u0A_FzKFfpeKsWVEra66V6lcmsph

###Project Name : Time Series Forecasting with LSTM: DOM Hourly Prediction
###Project Description : This project presents the implementation of Machine Learning to predict Time Series data using sequential model architecture with Long Short-Term Memory (LSTM). The dataset used is the Hourly DOM, which records observations at hourly intervals. The model is designed to understand patterns and trends in time series data to make accurate predictions. The use of a learning rate optimizer helps improve the efficiency and convergence speed of the model.
###Dataset : DOM hourly Dataset
###Tools : Google Colaboratory

**Created by : Baharuddin Nur Maulana**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.layers import Dense
from keras.layers import LSTM
from keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta

data_train = pd.read_csv('DOM_hourly.csv')
data_train.head()

data_train.isnull().sum()

def plot_series(time, series, format='-', start=0, end=None):
  plt.plot(time[start:end], series[start:end], format)
  plt.xlabel("Time")
  plt.ylabel("Value")
  plt.grid(True)

color_pal = ["#0000b3", "#0010d9", "#0020ff", "#0040ff", "#0060ff", "#0080ff", "#009fff", "#00bfff", "#00ffff"]
_ = data_train.plot(style='.', figsize=(15,5), color=color_pal[0], title='DOM')

data_train['Dates'] = pd.to_datetime(data_train['Datetime']).dt.date
data_train['Time'] = pd.to_datetime(data_train['Datetime']).dt.time
data_train.drop(columns=['Datetime'],inplace=True,axis=1)

data_train

data_train.set_index(data_train['Dates'],inplace=True)
data_train=data_train.drop(['Dates'],axis=1)

data_train

data_train=data_train.drop(['Time'],axis=1)

data_train

dataset = data_train.values
training_data_len = int(np.ceil( len(dataset) * .80))
training_data_len

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(dataset)
scaled_data

train_data = scaled_data[0:int(training_data_len),:]

x_train = []
y_train = []

for i in range(30, len(train_data)):
    x_train.append(train_data[i-30:i, 0])
    y_train.append(train_data[i, 0])

x_train, y_train = np.array(x_train), np.array(y_train)
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))

model = tf.keras.models.Sequential([
        tf.keras.layers.LSTM(128, return_sequences=True,input_shape= (x_train.shape[1], 1)),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.LSTM(128, return_sequences=False),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.Dense(64, activation="relu"),
        tf.keras.layers.Dense(1),
])

model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae') < 0.1 ):
      print("\n MAE < 10%")

  def on_train_end(self, epoch, logs={}):
    print('Done')

callbacks = myCallback()

Ir_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1.000e-04 * 10**(epoch/1))

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(x_train,y_train, epochs=100, batch_size=128, validation_split=0.2, shuffle=False, callbacks=[callbacks])

from matplotlib import pyplot as plt
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Mae Plot')
plt.ylabel('Mae')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Plot')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()